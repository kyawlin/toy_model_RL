{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07a1dc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00c4c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parameters import Tank_params, Disturbance_params, RL_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5717c168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tankEnv import Tank, TankEnv\n",
    "from utils import unpack_batch,RewardTracker,TBMeanTracker,ExperienceSourceFirstLast\n",
    "from agent import Agent, A2CNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "744bf305",
   "metadata": {},
   "outputs": [],
   "source": [
    "tank_0 = Tank(Tank_params, Disturbance_params)\n",
    "tank_1 = Tank(Tank_params, Disturbance_params)\n",
    "tank_2 = Tank(Tank_params, Disturbance_params)\n",
    "tank_3 = Tank(Tank_params, Disturbance_params)\n",
    "tank_4 = Tank(Tank_params, Disturbance_params)\n",
    "tank_5 = Tank(Tank_params, Disturbance_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a15b78",
   "metadata": {},
   "source": [
    "# one tank case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f40448e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tanks = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d0b2b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TankEnv(\n",
    "    [tank_0], Tank_params[\"max_lvl\"], Tank_params[\"min_lvl\"]\n",
    ")  # ,tank_2,tank_3,tank_4,tank_5,tank_6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ee26b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "090d5038",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_size = 12\n",
    "input_size = len(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9982b881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2CNet(\n",
      "  (policy): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=12, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Linear(in_features=12, out_features=2, bias=True)\n",
      "  )\n",
      "  (value): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=12, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Linear(in_features=12, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = A2CNet(input_size,n_hidden_size).to(RL_params[\"device\"])\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76aa09ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent =Agent(net,RL_params[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8944472",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_source = ExperienceSourceFirstLast(env, agent,  RL_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f3378fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=RL_params[\"learning_rate\"], eps=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0beb824",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(comment=\"1 tanks process\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0711b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done 500 games, mean reward 68.710 \n",
      " done 1000 games, mean reward 67.290 \n",
      " done 1500 games, mean reward 68.440 \n",
      " done 2000 games, mean reward 67.400 \n",
      "Solved !\n"
     ]
    }
   ],
   "source": [
    "batch = []\n",
    "\n",
    "with RewardTracker(writer, stop_reward=100) as tracker:\n",
    "    with TBMeanTracker(writer, batch_size=RL_params[\"batch_size\"]) as tb_tracker:\n",
    "        for step_idx, exp in enumerate(exp_source):\n",
    "            batch.append(exp)\n",
    "\n",
    "            new_rewards = exp_source.pop_total_rewards()\n",
    "            \n",
    "            if new_rewards:\n",
    "                if tracker.reward(new_rewards[0], step_idx):\n",
    "                    break\n",
    "            if exp.epi_step > 1000:\n",
    "                print(\"Solved ! @ epi_step\",exp.epi_step)\n",
    "                break\n",
    "\n",
    "            if len(batch) < RL_params[\"batch_size\"]:\n",
    "                continue\n",
    "\n",
    "            states_v, actions_t, vals_ref_v = unpack_batch(batch, net, RL_params)\n",
    "            batch.clear()\n",
    "            optimizer.zero_grad()\n",
    "            logits_v, value_v = net(states_v)\n",
    "            loss_value_v = F.mse_loss(value_v.squeeze(-1), vals_ref_v)\n",
    "\n",
    "            log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "\n",
    "            adv_v = vals_ref_v - value_v.squeeze(-1).detach()\n",
    "\n",
    "            log_prob_actions_v = adv_v[:, None] * log_prob_v[\n",
    "                actions_t.to(torch.bool)\n",
    "            ].view(RL_params[\"batch_size\"], -1)\n",
    "\n",
    "            loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "            prob_v = F.softmax(logits_v, dim=1)\n",
    "            entropy_loss_v = (\n",
    "                RL_params[\"entropy_beta\"] * (prob_v * log_prob_v).sum(dim=1).mean()\n",
    "            )\n",
    "            # calculate policy gradients only\n",
    "            loss_policy_v.backward(retain_graph=True)\n",
    "            grads = np.concatenate(\n",
    "                [\n",
    "                    p.grad.data.cpu().numpy().flatten()\n",
    "                    for p in net.parameters()\n",
    "                    if p.grad is not None\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # apply entropy and value gradients\n",
    "            loss_v = entropy_loss_v + loss_value_v\n",
    "            loss_v.backward()\n",
    "            nn_utils.clip_grad_norm_(net.parameters(), RL_params[\"clip_grad\"])\n",
    "            optimizer.step()\n",
    "            # get full loss\n",
    "            loss_v += loss_policy_v\n",
    "\n",
    "            tb_tracker.track(\"exp_step\", exp.epi_step, step_idx)\n",
    "\n",
    "            tb_tracker.track(\"advantage\", adv_v, step_idx)\n",
    "            tb_tracker.track(\"values\", value_v, step_idx)\n",
    "            tb_tracker.track(\"batch_rewards\", vals_ref_v, step_idx)\n",
    "            tb_tracker.track(\"loss_entropy\", entropy_loss_v, step_idx)\n",
    "            tb_tracker.track(\"loss_policy\", loss_policy_v, step_idx)\n",
    "            tb_tracker.track(\"loss_value\", loss_value_v, step_idx)\n",
    "            tb_tracker.track(\"loss_total\", loss_v, step_idx)\n",
    "            tb_tracker.track(\"grad_l2\", np.sqrt(np.mean(np.square(grads))), step_idx)\n",
    "            tb_tracker.track(\"grad_max\", np.max(np.abs(grads)), step_idx)\n",
    "            tb_tracker.track(\"grad_var\", np.var(grads), step_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "787c7a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kyaw/gitRepos/toy_model_RL/agent.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  states = torch.tensor(states, dtype=torch.float32).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 327 steps we got 319.000 reward\n"
     ]
    }
   ],
   "source": [
    "cur_steps = 0\n",
    "obs = env.reset()\n",
    "total_reward = 0.0\n",
    "total_steps = 0\n",
    "agent_states = None\n",
    "done = 0\n",
    "lvl = []\n",
    "while True:\n",
    "    obs = torch.FloatTensor([obs])\n",
    "    lvl.append((obs[0][0]).item())\n",
    "\n",
    "    actions, agent_states = agent(obs, agent_states)\n",
    "    obs, reward, is_done, info = env.step(actions, cur_steps)\n",
    "    cur_steps += 1\n",
    "    total_reward += sum(reward)\n",
    "    total_steps += 1\n",
    "    if is_done:\n",
    "        done += 1\n",
    "        obs = env.reset()\n",
    "    if done > 3: # three lives\n",
    "        break\n",
    "\n",
    "\n",
    "print(\"In %d steps we got %.3f reward\" % (total_steps, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adca9460",
   "metadata": {},
   "source": [
    "# six tanks case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d24db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "env6 = TankEnv(\n",
    "    [tank_0,tank_1,tank_2,tank_3,tank_4,tank_5], Tank_params[\"max_lvl\"], Tank_params[\"min_lvl\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8a5272b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2CNet(\n",
      "  (policy): Sequential(\n",
      "    (0): Linear(in_features=24, out_features=16, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Linear(in_features=16, out_features=12, bias=True)\n",
      "  )\n",
      "  (value): Sequential(\n",
      "    (0): Linear(in_features=24, out_features=16, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Linear(in_features=16, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "obs = env6.reset()\n",
    "n_hidden_size = 16\n",
    "input_size = len(obs)\n",
    "net6 = A2CNet(input_size,n_hidden_size).to(RL_params[\"device\"])\n",
    "print(net6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43af2fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent6 =Agent(net6,RL_params[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd2dc67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_source6 = ExperienceSourceFirstLast(env6, agent6,  RL_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df37d273",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer6 = torch.optim.Adam(net6.parameters(), lr=RL_params[\"learning_rate\"], eps=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d8d83e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer6 = SummaryWriter(comment=\"1 tanks process\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3316ae0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done 500 games, mean reward 42.663 \n",
      " done 1000 games, mean reward 42.050 \n",
      " done 1500 games, mean reward 44.980 \n",
      " done 2000 games, mean reward 42.925 \n",
      " done 2500 games, mean reward 43.380 \n",
      " done 3000 games, mean reward 43.838 \n",
      " done 3500 games, mean reward 43.887 \n",
      " done 4000 games, mean reward 43.752 \n",
      " done 4500 games, mean reward 43.177 \n",
      " done 5000 games, mean reward 43.612 \n",
      " done 5500 games, mean reward 42.987 \n",
      " done 6000 games, mean reward 44.295 \n",
      " done 6500 games, mean reward 45.385 \n",
      " done 7000 games, mean reward 44.652 \n",
      " done 7500 games, mean reward 43.693 \n",
      " done 8000 games, mean reward 44.692 \n",
      " done 8500 games, mean reward 44.832 \n",
      " done 9000 games, mean reward 45.002 \n",
      " done 9500 games, mean reward 46.598 \n",
      " done 10000 games, mean reward 44.102 \n",
      " done 10500 games, mean reward 48.135 \n",
      " done 11000 games, mean reward 45.673 \n",
      " done 11500 games, mean reward 47.233 \n",
      " done 12000 games, mean reward 45.780 \n",
      " done 12500 games, mean reward 47.665 \n",
      " done 13000 games, mean reward 44.808 \n",
      " done 13500 games, mean reward 47.247 \n",
      " done 14000 games, mean reward 46.682 \n",
      " done 14500 games, mean reward 47.547 \n",
      " done 15000 games, mean reward 47.378 \n",
      " done 15500 games, mean reward 48.302 \n",
      " done 16000 games, mean reward 48.147 \n",
      " done 16500 games, mean reward 49.407 \n",
      " done 17000 games, mean reward 49.417 \n",
      " done 17500 games, mean reward 50.762 \n",
      " done 18000 games, mean reward 50.450 \n",
      " done 18500 games, mean reward 49.118 \n",
      " done 19000 games, mean reward 51.398 \n",
      " done 19500 games, mean reward 51.565 \n",
      " done 20000 games, mean reward 53.595 \n",
      " done 20500 games, mean reward 53.368 \n",
      " done 21000 games, mean reward 51.590 \n",
      " done 21500 games, mean reward 52.382 \n",
      " done 22000 games, mean reward 53.943 \n",
      " done 22500 games, mean reward 53.655 \n",
      " done 23000 games, mean reward 53.537 \n",
      " done 23500 games, mean reward 54.955 \n",
      "Solved !\n"
     ]
    }
   ],
   "source": [
    "batch = []\n",
    "\n",
    "with RewardTracker(writer6, stop_reward=56) as tracker:\n",
    "    with TBMeanTracker(writer6, batch_size=RL_params[\"batch_size\"]) as tb_tracker:\n",
    "        for step_idx, exp in enumerate(exp_source6):\n",
    "            batch.append(exp)\n",
    "\n",
    "            new_rewards = exp_source6.pop_total_rewards()\n",
    "            \n",
    "            if new_rewards:\n",
    "                if tracker.reward(new_rewards[0], step_idx):\n",
    "                    break\n",
    "            if exp.epi_step > 1000:\n",
    "                print(\"Solved ! @ epi_step\",exp.epi_step)\n",
    "                break\n",
    "\n",
    "            if len(batch) < RL_params[\"batch_size\"]:\n",
    "                continue\n",
    "\n",
    "            states_v, actions_t, vals_ref_v = unpack_batch(batch, net6, RL_params)\n",
    "            batch.clear()\n",
    "            optimizer6.zero_grad()\n",
    "            logits_v, value_v = net6(states_v)\n",
    "            loss_value_v = F.mse_loss(value_v.squeeze(-1), vals_ref_v)\n",
    "\n",
    "            log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "\n",
    "            adv_v = vals_ref_v - value_v.squeeze(-1).detach()\n",
    "\n",
    "            log_prob_actions_v = adv_v[:, None] * log_prob_v[\n",
    "                actions_t.to(torch.bool)\n",
    "            ].view(RL_params[\"batch_size\"], -1)\n",
    "\n",
    "            loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "            prob_v = F.softmax(logits_v, dim=1)\n",
    "            entropy_loss_v = (\n",
    "                RL_params[\"entropy_beta\"] * (prob_v * log_prob_v).sum(dim=1).mean()\n",
    "            )\n",
    "            # calculate policy gradients only\n",
    "            loss_policy_v.backward(retain_graph=True)\n",
    "            grads = np.concatenate(\n",
    "                [\n",
    "                    p.grad.data.cpu().numpy().flatten()\n",
    "                    for p in net6.parameters()\n",
    "                    if p.grad is not None\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # apply entropy and value gradients\n",
    "            loss_v = entropy_loss_v + loss_value_v\n",
    "            loss_v.backward()\n",
    "            nn_utils.clip_grad_norm_(net6.parameters(), RL_params[\"clip_grad\"])\n",
    "            optimizer6.step()\n",
    "            # get full loss\n",
    "            loss_v += loss_policy_v\n",
    "\n",
    "            tb_tracker.track(\"exp_step\", exp.epi_step, step_idx)\n",
    "\n",
    "            tb_tracker.track(\"advantage\", adv_v, step_idx)\n",
    "            tb_tracker.track(\"values\", value_v, step_idx)\n",
    "            tb_tracker.track(\"batch_rewards\", vals_ref_v, step_idx)\n",
    "            tb_tracker.track(\"loss_entropy\", entropy_loss_v, step_idx)\n",
    "            tb_tracker.track(\"loss_policy\", loss_policy_v, step_idx)\n",
    "            tb_tracker.track(\"loss_value\", loss_value_v, step_idx)\n",
    "            tb_tracker.track(\"loss_total\", loss_v, step_idx)\n",
    "            tb_tracker.track(\"grad_l2\", np.sqrt(np.mean(np.square(grads))), step_idx)\n",
    "            tb_tracker.track(\"grad_max\", np.max(np.abs(grads)), step_idx)\n",
    "            tb_tracker.track(\"grad_var\", np.var(grads), step_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b5e7a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 35 steps we got 205.333 reward\n"
     ]
    }
   ],
   "source": [
    "cur_steps = 0\n",
    "obs = env6.reset()\n",
    "total_reward = 0.0\n",
    "total_steps = 0\n",
    "agent_states = None\n",
    "done = 0\n",
    "lvl = []\n",
    "while True:\n",
    "    obs = torch.FloatTensor([obs])\n",
    "    lvl.append((obs[0][0]).item())\n",
    "\n",
    "    actions, agent_states = agent6(obs, agent_states)\n",
    "    obs, reward, is_done, info = env6.step(actions, cur_steps)\n",
    "    cur_steps += 1\n",
    "    total_reward += sum(reward)\n",
    "    total_steps += 1\n",
    "    if is_done:\n",
    "        done += 1\n",
    "        obs = env6.reset()\n",
    "    if done > 3: # four lives\n",
    "        break\n",
    "\n",
    "\n",
    "print(\"In %d steps we got %.3f reward\" % (total_steps, total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cea5fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdaffd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
